{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03d5771-1b9b-46b5-9c0a-cb061581bee4",
   "metadata": {},
   "source": [
    "# PART 4 - Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b009cd-3346-4f2f-b5f3-55fedfe1c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoshichen/miniconda3/envs/nlp-learning/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561e8d58-6922-4423-9768-1c616d935bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 words:\n",
      "\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "\n",
      "len of words:  32033\n"
     ]
    }
   ],
   "source": [
    "# read in all names\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print('first 10 words:\\n')\n",
    "print(words[:10])\n",
    "print('\\nlen of words: ', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48735c1-5a26-48d0-bed8-40ac8fb97cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "vocab size:  27\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "chars = sorted(\n",
    "    list( set(''.join(words) ) )\n",
    ")\n",
    "stoi = {s:i+1 for i, s in enumerate(chars) }\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items() }\n",
    "print(itos)\n",
    "\n",
    "vocab_size = len(itos)\n",
    "print('vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e75a2750-18ae-41fb-be1c-044eac138bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    \n",
    "    X, Y = [], [] # inputs, targets\n",
    "    \n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context) # context words\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "# train/dev/test split\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9f85120-1fe0-46e4-ba6a-e00ed25555cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function for comparing handcraft gradient vs autograd from torch\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approx: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfaa69d8-9213-4b54-8b72-b62937fdb8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "params count:  4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # embedding dimension\n",
    "n_hidden = 64 # neuron size\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # reproducibility\n",
    "# embedding, 2d for each vocab, 27 total\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# layer 1, [context, hidden_size]=[3 word embedding, 100]=[6, 100]\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)*((n_embd * block_size)**-0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # keep bias, not needed but good to test gradient\n",
    "# layer 2, [100, 27], output softmax\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1 # 0 init value in bias could mask errors in gradient\n",
    "\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print('\\nparams count: ', sum(p.nelement() for p in parameters) ) # total number of params in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "715f9604-d29f-4bea-8b11-9d8f2fc5f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # shorter name for convenience\n",
    "\n",
    "# mini batch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf7dde0-8e67-49d8-b2f7-a277809af0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3482, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expanded FOWARD PASS\n",
    "# the expansion steps in chunks helps manual gradient calculation\n",
    "emb = C[Xb] # embed the chars into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concat to 1 embedding size of context\n",
    "\n",
    "# linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# batch norm layer, keep hidden state dist normal\n",
    "bnmeani = 1/n * hprebn.sum(0, keepdim=True) # batch avg of layer 1\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2 # diff squared\n",
    "bnvar = 1/(n-1) * (bndiff2).sum(0, keepdim=True) # avg variance, note: Bessels'correction, use n-1, not n\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5 # sqr root + epsilon to avoid divide by 0\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias # normalize\n",
    "\n",
    "# non-linearity activation\n",
    "h = torch.tanh(hpreact) # hidden state\n",
    "\n",
    "# linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss, same as F.cross_entroy(logits, Yb)\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp() # logits=log counts, exponentiate to get count\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum ** -1 # if use (1.0/count_sum) then cant get backprop to be exact\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# pytorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "          embcat, emb]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e427006-e316-4f1d-a7b0-47f7b1e3cac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "# excercise 1: back prop through the whole thing manually\n",
    "# all the variable defined in forward pass above 1 by 1\n",
    "\n",
    "# logprobs\n",
    "# logprobs shape [32, 27]\n",
    "print(logprobs.shape)\n",
    "\n",
    "# dlogprobs = dloss/dlogprobs = loss change due to all probs = same shape as logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a46b2b2-9769-4315-b404-88733d9a2a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeles (indices):\n",
      " tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
      "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18]) \n",
      "\n",
      "logprobs:\n",
      " tensor([[-2.6153, -2.4396, -4.0066, -2.9607, -3.9532, -2.4704, -3.7684, -3.3390,\n",
      "         -4.0580, -3.4449, -3.3145, -3.2976, -3.2953, -3.5589, -3.3670, -4.3238,\n",
      "         -4.7673, -3.9655, -4.2126, -2.9093, -2.9708, -3.8661, -3.7024, -2.6296,\n",
      "         -2.8401, -3.6538, -3.8384],\n",
      "        [-2.9221, -2.8495, -2.3383, -2.9125, -3.3366, -3.4310, -3.9924, -3.0359,\n",
      "         -3.9580, -3.7286, -2.9552, -3.0985, -3.0474, -3.5557, -3.0728, -3.2784,\n",
      "         -3.6525, -4.1385, -3.8847, -3.2946, -4.0144, -3.8155, -4.2485, -2.7364,\n",
      "         -3.7662, -3.3131, -3.7084],\n",
      "        [-3.9722, -3.7847, -4.2938, -4.4023, -3.8066, -3.1617, -2.8325, -2.7736,\n",
      "         -2.7888, -3.4810, -3.9044, -3.3975, -3.1220, -3.0080, -3.7930, -3.6750,\n",
      "         -4.3177, -3.3865, -3.5933, -2.1774, -2.6969, -3.2878, -3.1831, -3.2233,\n",
      "         -3.2701, -3.9411, -3.5760],\n",
      "        [-3.4586, -3.6900, -3.1528, -2.8812, -2.7921, -3.5659, -3.0457, -3.2006,\n",
      "         -3.0194, -4.0178, -3.2672, -3.7093, -3.4050, -3.1610, -2.7828, -2.7137,\n",
      "         -3.6929, -3.6994, -4.1183, -2.9949, -4.0569, -3.7733, -3.2631, -3.8793,\n",
      "         -3.6464, -3.0500, -3.0989],\n",
      "        [-4.1653, -4.1368, -3.7396, -3.7693, -4.0774, -3.1906, -3.3162, -4.0802,\n",
      "         -2.8917, -3.2581, -3.4086, -4.2249, -3.2622, -3.7623, -3.0687, -1.7315,\n",
      "         -3.9906, -2.9661, -3.0184, -2.7743, -3.2088, -4.2124, -3.8465, -4.2367,\n",
      "         -3.1057, -3.8011, -3.1177],\n",
      "        [-3.3400, -3.3528, -2.8934, -2.6933, -2.6983, -3.6532, -3.4935, -2.9022,\n",
      "         -3.6498, -4.2013, -3.1024, -4.0684, -3.6150, -3.1263, -2.7064, -3.0149,\n",
      "         -4.1074, -3.9069, -4.6539, -3.5406, -3.4960, -3.5197, -3.1468, -4.0663,\n",
      "         -4.0980, -2.4939, -3.1227],\n",
      "        [-2.8894, -4.5469, -2.7002, -4.2473, -3.5190, -3.9310, -2.7294, -2.9048,\n",
      "         -2.9916, -3.1162, -4.0579, -3.3615, -3.6266, -4.0391, -4.1852, -2.6461,\n",
      "         -2.4914, -3.1314, -4.6475, -3.4134, -3.6464, -2.9770, -3.0896, -3.8717,\n",
      "         -3.2234, -3.7332, -3.5037],\n",
      "        [-3.1397, -3.4680, -3.7365, -2.1020, -3.6191, -2.9386, -3.5140, -2.4777,\n",
      "         -3.2683, -4.7931, -3.5994, -4.2486, -3.9596, -2.8824, -4.0795, -4.2543,\n",
      "         -4.3108, -4.3083, -4.2234, -2.4052, -3.3707, -2.9906, -3.6701, -3.3700,\n",
      "         -3.7610, -2.8663, -3.3404],\n",
      "        [-3.0437, -3.5268, -4.0665, -3.2888, -4.6022, -3.2095, -2.8798, -2.4165,\n",
      "         -3.5842, -3.9227, -3.5406, -3.4601, -3.2603, -3.7474, -4.4791, -3.9758,\n",
      "         -3.9358, -3.2468, -3.8359, -2.3906, -3.3269, -2.4692, -3.1607, -3.0788,\n",
      "         -3.1344, -4.2067, -3.3899],\n",
      "        [-3.6245, -4.3294, -3.5559, -4.0465, -3.4589, -3.2918, -3.4001, -3.5175,\n",
      "         -2.3480, -2.7794, -4.3472, -3.3329, -3.2464, -3.1581, -3.7506, -3.5890,\n",
      "         -2.7394, -3.0638, -3.1447, -3.7134, -2.6596, -3.7077, -2.7357, -3.9507,\n",
      "         -3.4576, -3.3065, -4.0136],\n",
      "        [-4.2317, -4.9923, -3.2683, -3.1551, -3.4359, -3.3501, -4.0173, -5.1586,\n",
      "         -3.2931, -3.0787, -3.7029, -4.0267, -2.8367, -3.5357, -3.2202, -1.8833,\n",
      "         -2.9596, -2.8488, -2.5216, -3.3594, -3.1081, -4.6197, -4.4716, -3.7877,\n",
      "         -3.2823, -3.5963, -3.7686],\n",
      "        [-3.8279, -3.5232, -3.8435, -1.6111, -4.5076, -2.8583, -2.6581, -3.0990,\n",
      "         -3.2167, -3.5015, -4.2483, -3.9784, -3.8710, -3.6431, -5.0680, -5.0162,\n",
      "         -3.9006, -3.3205, -4.1781, -2.8662, -2.9498, -3.2659, -4.0160, -3.3195,\n",
      "         -3.0616, -3.1008, -4.1490],\n",
      "        [-4.0375, -4.3805, -3.0230, -4.2870, -3.6153, -4.1723, -2.4790, -3.2542,\n",
      "         -2.8121, -2.9648, -4.0954, -3.2790, -3.2434, -3.5395, -4.2454, -3.1778,\n",
      "         -2.5864, -3.0815, -3.1943, -3.7785, -3.0520, -3.2479, -2.3290, -4.6079,\n",
      "         -3.8497, -3.6873, -3.5830],\n",
      "        [-3.8394, -3.5996, -2.8968, -3.9277, -4.1479, -2.5945, -3.5780, -4.2586,\n",
      "         -3.5770, -3.0424, -3.5342, -3.3935, -2.6710, -3.3361, -2.9719, -3.0624,\n",
      "         -3.5568, -3.3860, -2.4685, -4.3126, -2.7636, -4.2043, -4.1164, -3.3275,\n",
      "         -3.3250, -3.2571, -3.2865],\n",
      "        [-3.0928, -3.6968, -3.5820, -2.9135, -4.0878, -3.3766, -3.4702, -2.8016,\n",
      "         -2.9284, -2.9935, -3.4562, -3.1344, -2.9798, -3.2342, -3.6157, -3.7550,\n",
      "         -3.8710, -2.8882, -3.5329, -2.9916, -2.9318, -2.9869, -3.6677, -3.3335,\n",
      "         -3.8147, -3.9003, -3.7888],\n",
      "        [-3.1644, -2.8126, -3.4317, -4.0569, -3.8999, -2.4552, -3.5515, -3.7733,\n",
      "         -3.5935, -2.9343, -3.3665, -3.0491, -2.7861, -4.1351, -3.5573, -4.1091,\n",
      "         -3.2037, -3.9835, -2.8324, -4.0704, -3.2776, -3.6813, -3.4209, -2.6853,\n",
      "         -2.8565, -3.5625, -3.8944],\n",
      "        [-3.5966, -3.7449, -2.4479, -3.0988, -2.7645, -2.9666, -4.1173, -3.6474,\n",
      "         -3.9653, -3.9839, -2.9749, -3.8541, -3.2307, -2.8807, -2.6702, -2.7434,\n",
      "         -4.2342, -4.0794, -3.7753, -3.1098, -3.6401, -4.1319, -4.7579, -3.0639,\n",
      "         -3.7714, -3.1090, -2.8596],\n",
      "        [-3.0233, -2.5311, -3.0094, -2.9242, -3.2220, -3.4066, -3.8076, -2.5661,\n",
      "         -3.6802, -4.1106, -2.6365, -4.3024, -3.7307, -3.9905, -3.1793, -3.5599,\n",
      "         -4.2588, -3.5254, -4.1142, -2.9226, -3.8886, -3.0479, -3.6356, -3.9774,\n",
      "         -3.7422, -3.1670, -2.7361],\n",
      "        [-4.0375, -4.3805, -3.0230, -4.2870, -3.6153, -4.1723, -2.4790, -3.2542,\n",
      "         -2.8121, -2.9648, -4.0954, -3.2790, -3.2434, -3.5395, -4.2454, -3.1778,\n",
      "         -2.5864, -3.0815, -3.1943, -3.7785, -3.0520, -3.2479, -2.3290, -4.6079,\n",
      "         -3.8497, -3.6873, -3.5830],\n",
      "        [-3.7599, -3.8569, -3.3867, -4.1412, -3.3124, -3.4137, -2.6593, -2.9647,\n",
      "         -3.3697, -3.3694, -3.5024, -3.0957, -2.7967, -2.9021, -4.3036, -3.7563,\n",
      "         -3.5692, -3.9615, -3.5158, -2.4128, -2.9951, -3.4239, -3.1897, -3.0123,\n",
      "         -3.4649, -3.8140, -3.6832],\n",
      "        [-2.9024, -4.3334, -4.0995, -3.2169, -3.4900, -3.3006, -3.5565, -3.2553,\n",
      "         -2.9176, -3.8602, -2.6779, -3.3212, -2.8031, -3.8551, -3.1305, -2.5758,\n",
      "         -4.1257, -3.8556, -3.2935, -2.9350, -3.9662, -3.5157, -3.7456, -3.7006,\n",
      "         -3.2914, -2.8526, -3.2222],\n",
      "        [-2.9453, -2.6139, -2.9423, -3.0337, -3.6974, -2.7900, -3.5017, -3.1339,\n",
      "         -3.7643, -4.1291, -2.9489, -3.6200, -3.7380, -3.4627, -3.1937, -3.1108,\n",
      "         -3.9747, -3.3193, -3.9541, -2.3774, -4.3915, -3.4402, -4.2809, -3.5018,\n",
      "         -3.3688, -3.5362, -3.3440],\n",
      "        [-4.0375, -4.3805, -3.0230, -4.2870, -3.6153, -4.1723, -2.4790, -3.2542,\n",
      "         -2.8121, -2.9648, -4.0954, -3.2790, -3.2434, -3.5395, -4.2454, -3.1778,\n",
      "         -2.5864, -3.0815, -3.1943, -3.7785, -3.0520, -3.2479, -2.3290, -4.6079,\n",
      "         -3.8497, -3.6873, -3.5830],\n",
      "        [-3.0537, -4.0618, -3.7106, -4.4573, -2.8276, -3.6659, -2.7130, -3.4849,\n",
      "         -3.7261, -3.1734, -3.4396, -3.2573, -3.3761, -3.4125, -3.2643, -3.0568,\n",
      "         -2.5733, -3.8332, -3.3125, -4.1898, -4.0642, -3.8584, -2.1325, -4.0254,\n",
      "         -2.8486, -3.4848, -4.1274],\n",
      "        [-3.4261, -2.5065, -3.8327, -3.0552, -3.6467, -2.7358, -3.7813, -3.5177,\n",
      "         -3.6483, -3.5845, -2.2766, -4.1252, -3.5355, -4.4792, -3.2658, -4.0266,\n",
      "         -4.1095, -4.2421, -4.4202, -3.6139, -4.3717, -3.8055, -4.5445, -1.8126,\n",
      "         -2.9907, -3.6979, -2.9724],\n",
      "        [-2.7135, -3.6067, -4.3058, -3.9891, -3.4646, -2.4747, -3.2353, -3.4548,\n",
      "         -3.6091, -3.3422, -3.4867, -2.9350, -2.8988, -3.3942, -3.9069, -4.3700,\n",
      "         -3.4295, -3.8747, -2.8368, -4.3619, -3.6743, -3.8817, -2.6495, -2.8865,\n",
      "         -2.6492, -3.1572, -4.5731],\n",
      "        [-3.1771, -3.7345, -3.9325, -4.6547, -3.0527, -3.8692, -2.4152, -3.2290,\n",
      "         -3.5646, -2.9712, -4.0888, -3.0416, -2.8191, -3.4918, -3.6859, -4.0587,\n",
      "         -2.8342, -3.6832, -3.5050, -3.3991, -3.8624, -3.3781, -1.9987, -3.9979,\n",
      "         -3.2194, -3.6363, -4.7052],\n",
      "        [-3.6644, -3.5946, -3.0056, -3.9312, -3.3576, -3.7945, -3.2643, -3.5525,\n",
      "         -3.2416, -2.9248, -3.9073, -3.5602, -3.2662, -3.3583, -3.1097, -2.7377,\n",
      "         -3.0604, -3.0487, -2.9974, -4.1470, -2.3037, -4.2206, -3.0837, -4.4919,\n",
      "         -4.0943, -3.3633, -3.0549],\n",
      "        [-2.7767, -3.2309, -4.1825, -3.9291, -3.4111, -2.7585, -3.1348, -3.2801,\n",
      "         -4.6437, -3.6630, -3.1490, -3.5354, -3.7027, -3.8950, -3.6139, -3.4439,\n",
      "         -2.5701, -3.6935, -2.7328, -3.6898, -3.4056, -3.4741, -3.1385, -2.7866,\n",
      "         -2.4789, -4.4729, -3.9758],\n",
      "        [-3.8850, -4.1342, -3.4507, -3.4476, -2.9537, -3.8261, -2.6553, -3.4380,\n",
      "         -2.6159, -3.4454, -3.5301, -3.3532, -3.4990, -3.2951, -3.4080, -2.7048,\n",
      "         -3.4987, -3.6298, -3.2871, -3.5601, -3.1595, -3.6371, -2.8575, -4.3186,\n",
      "         -3.4282, -2.9063, -3.2754],\n",
      "        [-3.3162, -2.9720, -3.7294, -2.7321, -3.1078, -2.9838, -3.3090, -3.4696,\n",
      "         -3.6438, -3.9808, -3.3153, -3.8523, -4.1025, -3.6529, -4.5366, -3.4251,\n",
      "         -3.3932, -3.5576, -3.1335, -2.8980, -2.8843, -3.6028, -3.8920, -2.6003,\n",
      "         -2.4477, -3.8836, -3.6234],\n",
      "        [-3.0720, -3.7913, -2.7013, -4.0373, -2.5587, -3.5756, -4.1382, -3.7421,\n",
      "         -4.1970, -2.8742, -3.5949, -3.4964, -3.4230, -2.4888, -3.1944, -3.2695,\n",
      "         -2.9726, -3.1618, -3.1384, -4.3680, -2.8891, -4.6429, -2.7726, -3.7896,\n",
      "         -3.9548, -3.2261, -3.9857]], grad_fn=<LogBackward0>) \n",
      "\n",
      "prob for each char in batch:\n",
      " tensor([-4.0580, -3.0728, -3.6750, -3.2631, -4.1653, -3.5406, -3.1162, -4.0795,\n",
      "        -3.2095, -4.3294, -3.1081, -1.6111, -2.8121, -2.9719, -2.9798, -3.1644,\n",
      "        -3.8541, -3.0233, -3.5830, -3.3694, -2.8526, -2.9453, -4.3805, -4.0618,\n",
      "        -3.5177, -2.8368, -2.9712, -3.9312, -2.7585, -3.4454, -3.3162, -3.1384],\n",
      "       grad_fn=<IndexBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('labeles (indices):\\n', Yb, '\\n') # labels\n",
    "print('logprobs:\\n', logprobs, '\\n') # [batch_size, vocabsize], prob for each word in batch\n",
    "print('prob for each char in batch:\\n', logprobs[range(n), Yb], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ade61a5-9c43-4ea4-bc15-3dd8efbd983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# loss = -(a + b + c) / 3 (for 3 numbers, or 32 in our batch)\n",
    "# = -a/3 + -b/3 + -c/3\n",
    "\n",
    "# dloss/da = -1/3, or -1/n, where n=batch size\n",
    "# dloss/d(other numbers) = 0, since the others dont participate in loss with respect to 'a'\n",
    "\n",
    "# init dlogprobs to 0\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "# for each label position, set derivative\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "\n",
    "# check with autograd\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f69f8b0a-a441-453f-b1c3-094c5e41ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprobs          | exact: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# probs = [log(a), log(b), log(c)\n",
    "# dloss/dprobs = dloss/dlogprobs * dlogprobs/dprobs = dlogprobs * [1/a, 0, 0]\n",
    "dprobs = dlogprobs * 1.0/probs\n",
    "\n",
    "cmp('dprobs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a36e838f-cfdb-4c08-8314-98c0ce109485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf413ad-8037-4040-b193-a15cb7b49977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
